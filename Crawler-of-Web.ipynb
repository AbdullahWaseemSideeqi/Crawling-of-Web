{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue \n",
    "import threading\n",
    "import random\n",
    "import time # for sleep\n",
    "import datetime # for getting current time\n",
    "import urllib.request # for fetching the content by hitting specific url\n",
    "from bs4 import BeautifulSoup # for parsing of the content got bby hitting a specific url\n",
    "import urllib.robotparser # for parsing robot.txt file and knowing which urls are acessible\n",
    "import io # for file opening\n",
    "\n",
    "\n",
    "# import os \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawler Parameters\n",
    "BACKQUEUES= 3\n",
    "THREADS= BACKQUEUES*3\n",
    "FRONTQUEUES= 5\n",
    "delay_seconds=20  # wait 20 seconds before fetching URLS from \n",
    "no_of_urls_processed=0 # to how many urls are currently being crawled\n",
    "exitFlag=False # to tell when all threads will exit\n",
    "maximum_urls_to_be_explored=1000 # to tell how many urls will be put in frontier before finishing\n",
    "pRioRity_Range=5 # how many front queues in frontier as one priority is assigned to one front queue\n",
    "queueLock = threading.Lock()\n",
    "\n",
    "# Add any other global parameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class url_with_domain:\n",
    "    def __init__(self,URL):\n",
    "        self.url=URL\n",
    "        self.domain=self._function_to_extract_domain()\n",
    "        self.robot_txt_url = \"https://\"+self.domain+\"/robots.txt\"\n",
    "        print(\"Main Url = \",self.url)\n",
    "        print(\"domain = \",self.domain)\n",
    "        print(\"domain of robot.txt = \",self.robot_txt_url)\n",
    "                \n",
    "    def _function_to_extract_domain(self):\n",
    "        dummy_url_without_https=self.url.split(\"https://\")\n",
    "        extracted_domain=dummy_url_without_https[1].split(\"/\")\n",
    "        \n",
    "        return extracted_domain[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FRONTIER\n",
    "\n",
    "Frontier should use the Mercator frontier design as discussed in lecture.\n",
    "\n",
    "Preferably it should be a class and should have the given functions.\n",
    "\n",
    "*prioritizer* function is a stub right now, it will return a random number  between 1 to f for given URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class frontier:\n",
    "        def __init__(self,n):\n",
    "            self.no_of_items_in_all_queus=n\n",
    "            self.F_q1=queue.Queue(n)\n",
    "            self.F_q2=queue.Queue(n)\n",
    "            self.F_q3=queue.Queue(n)\n",
    "            self.F_q4=queue.Queue(n)\n",
    "            self.F_q5=queue.Queue(n)\n",
    "            \n",
    "            #   for keeping the track of which back queue can get which domain\n",
    "            self.back_queue_domains=[\"N\"]*3\n",
    "            \n",
    "            # timer array keeping time_stamp for every back_queues\n",
    "            self.back_queue_timer_array=[None]*3\n",
    "            \n",
    "            # to keep the track of which urls have been explored\n",
    "            global maximum_urls_to_be_explored\n",
    "            self.list_of_fethched_urls=[None]* maximum_urls_to_be_explored\n",
    "            \n",
    "            # initially giving the time stamp(earliest time the request can be made)\n",
    "            #with difference of one second to successive queues \n",
    "            abc=0\n",
    "            for abc in range(3):\n",
    "                self.back_queue_timer_array[abc]=datetime.datetime.now()\n",
    "#                 print(back_queue_timer_array[i])\n",
    "                abc=abc+1\n",
    "                time.sleep(1)\n",
    "            \n",
    "            \n",
    "            self.b_q1=queue.Queue(n)\n",
    "            self.b_q2=queue.Queue(n)\n",
    "            self.b_q3=queue.Queue(n)\n",
    "            \n",
    "            self.seed_urls=[\"https://docs.oracle.com/en/\",\"https://www.oracle.com/corporate/\",\n",
    "                       \"https://en.wikipedia.org/wiki/Machine_learning\",\"https://www.csie.ntu.edu.tw/~cjlin/libsvm/index.html\"\n",
    "                      ,\"https://docs.oracle.com/middleware/jet210/jet/index.html\",\n",
    "                      \"https://en.wikipedia.org/w/api.php\",\"https://en.wikipedia.org/api/\",\n",
    "                       \"https://en.wikipedia.org/wiki/Weka_(machine_learning)\"]\n",
    "            \n",
    "        def check_time_stamp(self):\n",
    "            back_queue_number=0\n",
    "            if(self.back_queue_timer_array[0]<self.back_queue_timer_array[1] and self.back_queue_timer_array[0]<self.back_queue_timer_array[2]):\n",
    "                back_queue_number=1\n",
    "            elif(self.back_queue_timer_array[1]<self.back_queue_timer_array[0] and self.back_queue_timer_array[1]<self.back_queue_timer_array[2]):\n",
    "                back_queue_number=2\n",
    "            elif(self.back_queue_timer_array[2]<self.back_queue_timer_array[0] and self.back_queue_timer_array[2]<self.back_queue_timer_array[1]):\n",
    "                back_queue_number=3\n",
    "            return back_queue_number\n",
    "        def get_seed_urls_list(self):\n",
    "            return self.seed_urls\n",
    "        \n",
    "        def prioritizer(self,URL,f):\n",
    "            return random.randint(1,f)\n",
    "        \n",
    "        def assigning_url_to_front_queue(self,URL,f):\n",
    "            if(f==1):\n",
    "                if(self.F_q1.full()!=True):\n",
    "                    URL_with_domain_seprated=url_with_domain(URL)\n",
    "                    self.F_q1.put(URL_with_domain_seprated)\n",
    "            elif(f==2):\n",
    "                if(self.F_q2.full()!=True):\n",
    "                    URL_with_domain_seprated=url_with_domain(URL)\n",
    "                    self.F_q2.put(URL_with_domain_seprated)\n",
    "            elif(f==3):\n",
    "                if(self.F_q3.full()!=True):\n",
    "                    URL_with_domain_seprated=url_with_domain(URL)\n",
    "                    self.F_q3.put(URL_with_domain_seprated)          \n",
    "            elif(f==4):\n",
    "                if(self.F_q4.full()!=True):\n",
    "                    URL_with_domain_seprated=url_with_domain(URL)\n",
    "                    self.F_q4.put(URL_with_domain_seprated)\n",
    "            elif(f==5):\n",
    "                if(self.F_q5.full()!=True):\n",
    "                    URL_with_domain_seprated=url_with_domain(URL)\n",
    "                    self.F_q5.put(URL_with_domain_seprated)\n",
    "                    \n",
    "        def assigning_url_to_back_queue(self):\n",
    "            poped_object=url_with_domain(\"https://dummy_domain/asdj/dskasjd\")\n",
    "            if(self.b_q1.empty()==True or self.b_q2.empty()==True or self.b_q3.empty()==True  ):\n",
    "                random_no=random.randint(1,100)\n",
    "                print(\"random_no == \",random_no)\n",
    "                if(random_no>=1 and random_no<=40 and self.F_q1.empty()==False ):\n",
    "                    poped_object=self.F_q1.get()\n",
    "                elif (random_no>=41 and random_no<=70 and self.F_q2.empty()==False):\n",
    "                    poped_object=self.F_q2.get()\n",
    "                elif (random_no>=71 and random_no<=85 and self.F_q3.empty()==False):\n",
    "                    poped_object=self.F_q3.get()\n",
    "                elif (random_no>=86 and random_no<=95 and self.F_q4.empty()==False):\n",
    "                    poped_object=self.F_q4.get()\n",
    "                elif (random_no>=96 and random_no<=100 and self.F_q5.empty()==False):\n",
    "                    poped_object=self.F_q5.get()\n",
    "                    \n",
    "                extracter_domain_of_URL_popped_from_F_Queue=poped_object._function_to_extract_domain()\n",
    "                # to check whether the domain of popped url from front queue matches the \n",
    "                # domain of back queue 1\n",
    "#                 print(self.back_queue_domains)\n",
    "                if( self.back_queue_domains[0] == extracter_domain_of_URL_popped_from_F_Queue):\n",
    "                        self.b_q1.put(poped_object)\n",
    "                # to check whether the domain of popped url from front queue matches the \n",
    "                # domain of back queue 2\n",
    "                elif( self.back_queue_domains[1] == extracter_domain_of_URL_popped_from_F_Queue):\n",
    "                        self.b_q2.put(poped_object)\n",
    "                # to check whether the domain of popped url from front queue matches the \n",
    "                # domain of back queue 3\n",
    "                elif( self.back_queue_domains[2] == extracter_domain_of_URL_popped_from_F_Queue):\n",
    "                        self.b_q3.put(poped_object)\n",
    "                        \n",
    "                #  if first back queue is empty\n",
    "                elif(self.b_q1.empty()==True and (extracter_domain_of_URL_popped_from_F_Queue!=\"dummy_domain\") ):\n",
    "                    self.b_q1.put(poped_object)\n",
    "                    self.back_queue_domains[0]=poped_object._function_to_extract_domain()\n",
    "\n",
    "                \n",
    "                #  if second back queue is empty\n",
    "                elif(self.b_q2.empty()==True and (extracter_domain_of_URL_popped_from_F_Queue!=\"dummy_domain\") ):\n",
    "                    self.b_q2.put(poped_object)\n",
    "                    self.back_queue_domains[1]=poped_object._function_to_extract_domain()\n",
    "\n",
    "                #   if third back queue is empty\n",
    "                elif(self.b_q3.empty()==True and (extracter_domain_of_URL_popped_from_F_Queue!=\"dummy_domain\") ):                \n",
    "                    self.b_q3.put(poped_object)\n",
    "                    self.back_queue_domains[2]=poped_object._function_to_extract_domain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching and Parsing the content of Url got from frontier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "#fetching module and parsing module combined \n",
    "    \n",
    "def _function_to_extract_domain_same_function(url):\n",
    "    dummy_url_without_https=url.split(\"https://\")\n",
    "    extracted_domain=dummy_url_without_https[1].split(\"/\")\n",
    "        \n",
    "    return extracted_domain[0]\n",
    "\n",
    "# function for saving the content recieved by hitting every url respectively\n",
    "# it will be stored in txt file \n",
    "# with the first line containing the url which is hit to get the content \n",
    "# and the rest of line will contain the content\n",
    "# Moreover the name of the file would be the serial number of the url that is being crawled\n",
    "def creation_of_database(UrL,n_urls_processed,soup):\n",
    "    filename = str(n_urls_processed) + \".txt\"\n",
    "    s=soup.get_text()\n",
    "    with io.open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(UrL)\n",
    "        f.write(s)  \n",
    "        f.close() #closing the file\n",
    "    \n",
    "def fetching_content_by_hitting_url(UrL,n_urls_processed):\n",
    "\n",
    "    link_list=[None]*5000\n",
    "    n_links_processed=0\n",
    "    fhand= urllib.request.urlopen(UrL)\n",
    "    domain = _function_to_extract_domain_same_function(UrL)\n",
    "    # print(\"domain == \",domain)\n",
    "    # for line in fhand:\n",
    "    #     print(line.decode().strip())\n",
    "\n",
    "    # print()\n",
    "    # print(\"____________________________________________________________________________________\")\n",
    "    # print()\n",
    "    data = fhand.read()\n",
    "\n",
    "    # print(data.title)\n",
    "    # print(data)\n",
    "\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    # print(soup.prettify())\n",
    "    # if the file is of not the type xml\n",
    "    if (soup.find(\"xml\")==None):\n",
    "        \n",
    "        creation_of_database(UrL,n_urls_processed,soup)\n",
    "#         i=0\n",
    "        for link in soup.find_all( 'link' ):\n",
    "            actual_link=link.get('href')\n",
    "            if(actual_link.find(\".css\")==-1 and actual_link.find(\".js\")==-1 and actual_link.find(\".png\")==-1 ):\n",
    "                # for checking if the link is relative\n",
    "                if (actual_link.find(\"https://\")==-1):\n",
    "\n",
    "                    actual_link=\"https://\"+ domain + actual_link # making the link complete\n",
    "                    link_list[n_links_processed]=actual_link\n",
    "                    n_links_processed = n_links_processed + 1\n",
    "#                     print(i,\"    \",actual_link)\n",
    "                else:\n",
    "                    link_list[n_links_processed]=actual_link\n",
    "                    n_links_processed = n_links_processed + 1\n",
    "#                     print(i,\"    without appending\",actual_link)\n",
    "#             i=i+1\n",
    "        # for keeping a check for the end of elements in the below list of urls\n",
    "        link_list[n_links_processed]=\"-1\"\n",
    "\n",
    "    return link_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILTER URLS\n",
    "Filter the URLS that are in robots.txt files of server and the have been already processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# url filtering module on the basis of robot.txt file\n",
    "\n",
    "\n",
    "def to_check_which_urls_are_already_crawled(list_Of_Filtered_urls, no_of_urls_crawled,list_of_crawled_urls):\n",
    "    ultimate_filtered_list = [None]*(len(list_Of_Filtered_urls))\n",
    "    \n",
    "    outer_iter=0\n",
    "    inner_iter=0\n",
    "    ultimate_Filter_iter=0\n",
    "    check=False\n",
    "    while (outer_iter < (len(list_Of_Filtered_urls))):\n",
    "        inner_iter = 0\n",
    "        while (inner_iter <= no_of_urls_crawled):\n",
    "            if(list_Of_Filtered_urls[outer_iter]==list_of_crawled_urls[inner_iter]):\n",
    "                check=True\n",
    "            inner_iter = inner_iter + 1\n",
    "            \n",
    "        if(check==False):\n",
    "            ultimate_filtered_list[ultimate_Filter_iter] = list_Of_Filtered_urls[outer_iter]\n",
    "            ultimate_Filter_iter = ultimate_Filter_iter + 1\n",
    "        else:\n",
    "            check=False\n",
    "        outer_iter = outer_iter + 1\n",
    "    ultimate_filtered_list[ultimate_Filter_iter] = \"-1\"\n",
    "    return ultimate_filtered_list\n",
    "    \n",
    "\n",
    "def function_for_url_filtering_on_the_basis_of_robot_file(list_of_fetched_urls,Robot_txt_URL):\n",
    "    rp = urllib.robotparser.RobotFileParser()\n",
    "    rp.set_url(Robot_txt_URL)\n",
    "    rp.read()\n",
    "\n",
    "    # rrate = rp.request_rate(\"*\")\n",
    "    # print(\" rrate.requests == \",rrate.requests )\n",
    "    # print(\" rrate.seconds == \",rrate.seconds ) \n",
    "    # print (\" rp.crawl_delay == \",rp.crawl_delay(\"*\") )\n",
    "    \n",
    "    # loop to know how many urls in the fetched list\n",
    "    no_of_items_in_list_fetched=0\n",
    "    while (list_of_fetched_urls[no_of_items_in_list_fetched] != \"-1\"):\n",
    "        no_of_items_in_list_fetched = no_of_items_in_list_fetched + 1\n",
    "    \n",
    "    filtered_url_list = [None] * no_of_items_in_list_fetched # list which will contain the filtered urls\n",
    "    iter_filter_list= 0 # iterator pf filtered list\n",
    "    ghj=0\n",
    "    while (list_of_fetched_urls[ghj] != \"-1\" ):\n",
    "        url_check = str(list_of_fetched_urls[ghj])\n",
    "        if(rp.can_fetch(\"*\", url_check )==True):\n",
    "            filtered_url_list[iter_filter_list] = list_of_fetched_urls[ghj]\n",
    "            iter_filter_list = iter_filter_list + 1\n",
    "        ghj = ghj +1\n",
    "    \n",
    "    return filtered_url_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --Write any other codes/data require to run the crawler above this cell------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "list_of_URLS_returned_by_fetch_and_parse_module=[None]*5000\n",
    "\n",
    "class myThread (threading.Thread):\n",
    "    def __init__(self, threadID, name, f):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.threadID = threadID\n",
    "        self.name = name\n",
    "        self.Frontier = f\n",
    "        self.URL=[None]*1\n",
    "    def run(self):\n",
    "        global no_of_urls_processed\n",
    "        print (\"Starting \" , self.name)\n",
    "        request_url(self.name, self.Frontier,self.URL[0],no_of_urls_processed)\n",
    "        print (\"Exiting \" , self.name)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def request_url(threadName, Frontier,URL,no_of_urls_processed):\n",
    "    Back_queue_no=0\n",
    "    global exitFlag,delay_seconds,pRioRity_Range,list_of_URLS_returned_by_fetch_and_parse_module,queueLock\n",
    "    while (exitFlag==False):\n",
    "        queueLock.acquire()\n",
    "        if ( Frontier.b_q1.empty()==False or Frontier.b_q2.empty()==False or Frontier.b_q3.empty()==False ):\n",
    "            Back_queue_no=Frontier.check_time_stamp()\n",
    "            if(Back_queue_no == 1): # means first back queue time stamp is least\n",
    "                current_time=datetime.datetime.now()\n",
    "                if(Frontier.back_queue_timer_array[0]<=current_time):#means the time stamp of first back queue is less or\n",
    "                                                                            # equal to current time, hence thread can get url\n",
    "                                                                            # from it.\n",
    "                    URL=Frontier.b_q1.get()\n",
    "                                                            \n",
    "                    # __________________for keeping the track of how many urls have been processed______________ #\n",
    "                    Frontier.list_of_fethched_urls[no_of_urls_processed]= URL.url                    \n",
    "                    no_of_urls_processed = no_of_urls_processed + 1\n",
    "                    \n",
    "                    #____hitting the url and getting more url by parsing the content got by hitting this urls___#\n",
    "                    list_of_URLS_returned_by_fetch_and_parse_module= fetching_content_by_hitting_url(URL.url, ( no_of_urls_processed-1) )\n",
    "                    \n",
    "                    #___________Url filtering on the basis of robot_txt_file___________________________________#\n",
    "                    list_of_filtered_urls=function_for_url_filtering_on_the_basis_of_robot_file(list_of_URLS_returned_by_fetch_and_parse_module,URL.robot_txt_url)\n",
    "                    \n",
    "                    #__________for checking of duplicate url comes that is already being crawled______________#\n",
    "                    Ultimate_Filtered_list_of_urls  = to_check_which_urls_are_already_crawled(list_of_filtered_urls, (no_of_urls_processed - 1) ,Frontier.list_of_fethched_urls)\n",
    "                    \n",
    "                    \n",
    "                    #__________putting_filterd_urls_in_front_queue_________________________________________#\n",
    "                    you=0\n",
    "                    while(Ultimate_Filtered_list_of_urls[you]!=\"-1\"):\n",
    "                        Prioority=Frontier.prioritizer(Ultimate_Filtered_list_of_urls[you],pRioRity_Range)\n",
    "                        Frontier.assigning_url_to_front_queue(Ultimate_Filtered_list_of_urls[you],Prioority)\n",
    "                                                            \n",
    "                    \n",
    "                    # ________________ Module to check wether after the thread have taken Url from back queue 1 \n",
    "                    # whether the back queue 1 gets empty if so then keep on trying to take some url to the \n",
    "                    # back queue 1 until it no Longer remains empty _____________________________________________ #\n",
    "                    while(Frontier.b_q1.empty()==True):\n",
    "                        Frontier.assigning_url_to_back_queue()\n",
    "                    #________________________________________________________________________________________#\n",
    "                    \n",
    "                    print(\"URl is popped from Back Queue 1.\")\n",
    "                    print(\"Url == \",URL.url)\n",
    "                    print(\"Url domain == \",URL.domain)\n",
    "                    Frontier.back_queue_timer_array[0]= datetime.datetime.now() + datetime.timedelta(0,delay_seconds) # adding certain seconds\n",
    "                    \n",
    "                    queueLock.release()\n",
    "                else:\n",
    "                    queueLock.release()\n",
    "            elif (Back_queue_no == 2): # means second back queue time stamp is least\n",
    "                current_time=datetime.datetime.now()\n",
    "                if(Frontier.back_queue_timer_array[1]<=current_time):#means the time stamp of second back queue is less or\n",
    "                                                                            # equal to current time, hence thread can get url\n",
    "                                                                            # from it.\n",
    "                    URL=Frontier.b_q2.get()\n",
    "                    \n",
    "                    # __________________for keeping the track of how many urls have been processed______________ #\n",
    "                    Frontier.list_of_fethched_urls[no_of_urls_processed]= URL.url                    \n",
    "                    no_of_urls_processed = no_of_urls_processed + 1\n",
    "                    \n",
    "                    #____hitting the url and getting more url by parsing the content got by hitting this urls___#\n",
    "                    list_of_URLS_returned_by_fetch_and_parse_module= fetching_content_by_hitting_url(URL.url,  ( no_of_urls_processed-1) )\n",
    "                    \n",
    "                    #___________Url filtering on the basis of robot_txt_file___________________________________#\n",
    "                    list_of_filtered_urls=function_for_url_filtering_on_the_basis_of_robot_file(list_of_URLS_returned_by_fetch_and_parse_module,URL.robot_txt_url)\n",
    "                    \n",
    "                    #__________for checking of duplicate url comes that is already being crawled______________#\n",
    "                    Ultimate_Filtered_list_of_urls  = to_check_which_urls_are_already_crawled(list_of_filtered_urls, (no_of_urls_processed - 1) ,Frontier.list_of_fethched_urls)\n",
    "                    \n",
    "                    \n",
    "                    #__________putting_filterd_urls_in_front_queue_________________________________________#\n",
    "                    you=0\n",
    "                    while(Ultimate_Filtered_list_of_urls[you]!=\"-1\"):\n",
    "                        Prioority=Frontier.prioritizer(Ultimate_Filtered_list_of_urls[you],pRioRity_Range)\n",
    "                        Frontier.assigning_url_to_front_queue(Ultimate_Filtered_list_of_urls[you],Prioority)\n",
    "                    \n",
    "                    # ________________ Module to check wether after the thread have taken Url from back queue 2 \n",
    "                    # whether the back queue 2 gets empty if so then keep on trying to take some url to the \n",
    "                    # back queue 2 until it no Longer remains empty _____________________________________________ #\n",
    "                    while(Frontier.b_q2.empty()==True):\n",
    "                        Frontier.assigning_url_to_back_queue()\n",
    "                    #________________________________________________________________________________________#\n",
    "                    \n",
    "                    print(\"URl is popped from Back Queue 2.\")\n",
    "                    print(\"Url == \",URL.url)\n",
    "                    print(\"Url domain == \",URL.domain)\n",
    "                    Frontier.back_queue_timer_array[1]= datetime.datetime.now() + datetime.timedelta(0,delay_seconds) # adding certain seconds\n",
    "                    \n",
    "                    queueLock.release()\n",
    "                else:\n",
    "                    queueLock.release()\n",
    "            elif (Back_queue_no == 3): # means third back queue time stamp is least\n",
    "                current_time=datetime.datetime.now()\n",
    "                if(Frontier.back_queue_timer_array[2]<=current_time):#means the time stamp of first back queue is less or\n",
    "                                                                            # equal to current time, hence thread can get url\n",
    "                                                                            # from it.\n",
    "                    URL=Frontier.b_q3.get()\n",
    "                                                            \n",
    "                    # __________________for keeping the track of how many urls have been processed______________ #\n",
    "                    Frontier.list_of_fethched_urls[no_of_urls_processed]= URL.url                    \n",
    "                    no_of_urls_processed = no_of_urls_processed + 1\n",
    "                    \n",
    "                    #____hitting the url and getting more url by parsing the content got by hitting this urls___#\n",
    "                    list_of_URLS_returned_by_fetch_and_parse_module= fetching_content_by_hitting_url(URL.url ,  ( no_of_urls_processed-1) )\n",
    "                    \n",
    "                    #___________Url filtering on the basis of robot_txt_file___________________________________#\n",
    "                    list_of_filtered_urls=function_for_url_filtering_on_the_basis_of_robot_file(list_of_URLS_returned_by_fetch_and_parse_module,URL.robot_txt_url)\n",
    "                    \n",
    "                    #__________for checking of duplicate url comes that is already being crawled______________#\n",
    "                    Ultimate_Filtered_list_of_urls  = to_check_which_urls_are_already_crawled(list_of_filtered_urls, (no_of_urls_processed - 1) ,Frontier.list_of_fethched_urls)\n",
    "                    \n",
    "                    \n",
    "                    #__________putting_filterd_urls_in_front_queue_________________________________________#\n",
    "                    you=0\n",
    "                    while(Ultimate_Filtered_list_of_urls[you]!=\"-1\"):\n",
    "                        Prioority=Frontier.prioritizer(Ultimate_Filtered_list_of_urls[you],pRioRity_Range)\n",
    "                        Frontier.assigning_url_to_front_queue(Ultimate_Filtered_list_of_urls[you],Prioority)\n",
    "                                        \n",
    "                                        \n",
    "                    # ________________ Module to check wether after the thread have taken Url from back queue 3 \n",
    "                    # whether the back queue 3 gets empty if so then keep on trying to take some url to the \n",
    "                    # back queue 3 until it no Longer remains empty _____________________________________________ #\n",
    "                    while(Frontier.b_q3.empty()==True):\n",
    "                        Frontier.assigning_url_to_back_queue()\n",
    "                    #________________________________________________________________________________________#\n",
    "                    \n",
    "                    print(\"URl is popped from Back Queue 3.\")\n",
    "                    print(\"Url == \",URL.url)\n",
    "                    print(\"Url domain == \",URL.domain)\n",
    "                    Frontier.back_queue_timer_array[2]= datetime.datetime.now() + datetime.timedelta(0,delay_seconds) # adding certain seconds\n",
    "                    \n",
    "                    queueLock.release()\n",
    "                else:\n",
    "                    queueLock.release()\n",
    "            else:\n",
    "                queueLock.release()\n",
    "        else:\n",
    "            queueLock.release()\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main from where object where thread will be created and started and then populate frontier by crawling over the fetched Urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Starting   Thread-1\n",
      "Starting  Thread-3\n",
      "Thread-2\n",
      "Starting  Thread-4Starting  Thread-5\n",
      "Starting \n",
      " Thread-6\n",
      "Starting  Thread-7\n",
      "Starting  Thread-8\n",
      "Starting  Thread-9\n"
     ]
    }
   ],
   "source": [
    "global no_of_urls_processed,exitFlag,queueLock\n",
    "\n",
    "threadList = [\"Thread-1\", \"Thread-2\", \"Thread-3\", \"Thread-4\", \"Thread-5\", \"Thread-6\",\"Thread-7\", \"Thread-8\", \"Thread-9\"]\n",
    "nameList = [\"One\", \"Two\", \"Three\", \"Four\", \"Five\"]\n",
    "\n",
    "\n",
    "# workQueue = Queue.Queue(10)\n",
    "threads = []\n",
    "threadID = 1\n",
    "Frontier_1=frontier(100)\n",
    "\n",
    "# Create new threads\n",
    "for tName in threadList:\n",
    "    thread = myThread(threadID, tName, Frontier_1)\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "    threadID += 1\n",
    "        \n",
    "# Fill the queue\n",
    "queueLock.acquire()\n",
    "\n",
    "seed_urls=Frontier_1.get_seed_urls_list()\n",
    "ii=1\n",
    "for seed_url in seed_urls:\n",
    "    priority=Frontier_1.prioritizer(seed_url,pRioRity_Range)\n",
    "    print(ii,\"    Priority of seed url \",seed_url,\"  == \",priority)\n",
    "    Frontier_1.assigning_url_to_front_queue(seed_url,priority)\n",
    "#     Frontier_1.assigning_url_to_back_queue()\n",
    "    ii=ii+1\n",
    "            \n",
    "kk=0\n",
    "for kk in range(10):\n",
    "    Frontier_1.assigning_url_to_back_queue()\n",
    "    kk=kk+1\n",
    "\n",
    "    \n",
    "queueLock.release()\n",
    "\n",
    "# Wait for queue to empty\n",
    "while (no_of_urls_processed<=10):\n",
    "    pass\n",
    "\n",
    "# Notify threads it's time to exit\n",
    "exitFlag = True\n",
    "\n",
    "# Wait for all threads to complete\n",
    "for t in threads:\n",
    "    t.join()\n",
    "print (\"Exiting Main Thread\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------End of Notebook-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
